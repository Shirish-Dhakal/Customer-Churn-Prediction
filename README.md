# ğŸ”„ Customer Churn Prediction

This project aims to predict customer churn using machine learning, enabling businesses to proactively retain customers by identifying those at risk of leaving. The pipeline includes **data cleaning**, **exploratory analysis**, and model building using several algorithms, with **XGBoost** selected as the final model.

---

## ğŸ“Œ Project Highlights

- Performed thorough **data cleaning**, handling missing values and encoding categorical features.
- Conducted **EDA** to understand churn patterns and feature distributions.
- Trained multiple models including Logistic Regression, Random Forest, and XGBoost.
- Selected **XGBoost** based on its superior performance across metrics.
- Achieved **99.68% accuracy**, **99.7% precision**, **98.8% recall**, and **99.2% F1-score**.
- Used a **confusion matrix** and model probability analysis for validation.
- Provided business insights and actionable recommendations based on predictions.

---

## ğŸ“Š Exploratory Data Analysis (EDA)

- Analyzed churn distribution across demographics and usage patterns.
- Visualized key features such as login frequency, spending, and recency.
- Identified correlations and removed redundant or low-importance variables.

---

## ğŸ§¹ Data Cleaning

- Removed duplicate and inconsistent records.
- Imputed missing values using statistical methods.
- Converted categorical variables using one-hot encoding.
- Normalized continuous variables for models like Logistic Regression.

---


## ğŸ“Š Model Performance (XGBoost)

| Metric     | Value     |
|------------|-----------|
| Accuracy   | 99.68%    |
| Precision  | 99.7%     |
| Recall     | 98.8%     |
| F1 Score   | 99.2%     |

---

## ğŸ’¼ Business Applications

- ğŸ¯ **Targeted Retention** â€“ Intervene early for at-risk customers.
- ğŸ“ **Customer Support Prioritization** â€“ Focus on high-risk profiles.
- ğŸ’° **CLV-Based Segmentation** â€“ Combine risk scores with value tiers.
- ğŸ“‰ **Churn Insights** â€“ Identify trends driving attrition and refine service.

---

## ğŸš€ Potential Improvements

- Enhanced **feature engineering** (e.g., session trends, time-based patterns)
- Hyperparameter tuning using **Grid Search** or **Bayesian Optimization**
- **Model explainability** using SHAP for transparency
- **Ensemble blending** with Logistic Regression, XGBoost, and RF

---

## ğŸ› ï¸ Tools & Libraries

- Python (Jupyter Notebook)
- Pandas, NumPy, Matplotlib, Seaborn
- Scikit-learn, XGBoost

---
